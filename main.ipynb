{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 13 persons, 2 chairs, 1 dining table, 2 laptops, 251.6ms\n",
      "Speed: 5.0ms preprocess, 251.6ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "# Load YOLOv8 model (pretrained on COCO dataset)\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "# Load image\n",
    "image_path = \"test_2.jpg\"  # Replace with your image path\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Perform object detection\n",
    "results = model(image)\n",
    "\n",
    "# Process results\n",
    "for result in results:\n",
    "    for box in result.boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])  # Bounding box coordinates\n",
    "        conf = box.conf[0].item()  # Confidence score\n",
    "        cls = int(box.cls[0].item())  # Class index\n",
    "        label = model.names[cls]  # Class name\n",
    "\n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(image, f\"{label} {conf:.2f}\", (x1, y1 - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "# Show output image\n",
    "\n",
    "image_resized = cv2.resize(image, (700, 700))\n",
    "\n",
    "# Show resized image\n",
    "cv2.imshow(\"YOLOv8 Detection (300x300)\", image_resized)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load YOLOv8 model (pretrained on COCO dataset)\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(1)  # Change to 1 or 2 if using an external webcam\n",
    "\n",
    "# Set capture interval (every 2 seconds)\n",
    "capture_interval = 2\n",
    "last_capture_time = time.time()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()  # Read frame from webcam\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Failed to capture frame\")\n",
    "        break\n",
    "\n",
    "    # Check if 2 seconds have passed\n",
    "    current_time = time.time()\n",
    "    if current_time - last_capture_time >= capture_interval:\n",
    "        last_capture_time = current_time  # Reset timer\n",
    "        \n",
    "        # Perform YOLOv8 detection\n",
    "        results = model(frame)\n",
    "\n",
    "        # Process results and draw bounding boxes\n",
    "        for result in results:\n",
    "            for box in result.boxes:\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])  # Bounding box coordinates\n",
    "                conf = box.conf[0].item()  # Confidence score\n",
    "                cls = int(box.cls[0].item())  # Class index\n",
    "                label = model.names[cls]  # Class name\n",
    "\n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, f\"{label} {conf:.2f}\", (x1, y1 - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Show frame\n",
    "    cv2.imshow(\"YOLOv8 Live Detection\", frame)\n",
    "\n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import threading\n",
    "import queue\n",
    "import speech_recognition as sr\n",
    "import pyttsx3\n",
    "from ultralytics import YOLO\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Constants\n",
    "YOLO_MODEL = 'yolov8n.pt'\n",
    "RELEVANT_CLASSES = ['cell phone', 'bottle', 'laptop', 'mouse']\n",
    "CALIBRATION_DISTANCE = 1.0  # meters\n",
    "FRAME_SIZE = 384  # Common size for both models\n",
    "\n",
    "# Global variables\n",
    "latest_frame = None\n",
    "scaling_factor = None\n",
    "frame_queue = queue.Queue(maxsize=1)\n",
    "\n",
    "# Initialize Text-to-Speech\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Load models\n",
    "yolo_model = YOLO(YOLO_MODEL)\n",
    "depth_estimator = pipeline('depth-estimation', model='Intel/dpt-hybrid-midas')\n",
    "\n",
    "def camera_capture():\n",
    "    cap = cv2.VideoCapture(1)\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            resized_frame = cv2.resize(frame, (FRAME_SIZE, FRAME_SIZE))\n",
    "            if frame_queue.full():\n",
    "                frame_queue.get_nowait()\n",
    "            frame_queue.put(resized_frame)\n",
    "\n",
    "def process_frame():\n",
    "    global scaling_factor\n",
    "    while True:\n",
    "        frame = frame_queue.get()\n",
    "\n",
    "        # Convert BGR (OpenCV) to RGB for transformer model\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_pil = Image.fromarray(frame_rgb)\n",
    "\n",
    "        # Object Detection\n",
    "        results = yolo_model(frame)[0]\n",
    "        detections = []\n",
    "        for box in results.boxes:\n",
    "            class_id = int(box.cls)\n",
    "            label = yolo_model.names[class_id]  # Fix applied\n",
    "            if label in RELEVANT_CLASSES:\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                detections.append((label, (x1, y1, x2, y2)))\n",
    "\n",
    "        # Depth Estimation\n",
    "        depth_result = depth_estimator(frame_pil)\n",
    "        depth_map = depth_result['depth']  # Fix applied\n",
    "\n",
    "        # Convert to numpy if needed\n",
    "        depth_map = np.array(depth_map)\n",
    "\n",
    "        # Process detections\n",
    "        feedback = []\n",
    "        for label, bbox in detections:\n",
    "            x1, y1, x2, y2 = bbox\n",
    "            depth_roi = depth_map[y1:y2, x1:x2]\n",
    "            avg_depth = depth_roi.mean() if depth_roi.size > 0 else 0\n",
    "\n",
    "            if scaling_factor and avg_depth > 0:\n",
    "                distance = 1 / (avg_depth * scaling_factor)\n",
    "                feedback.append(f\"{label} {distance:.1f} meters away\")\n",
    "            else:\n",
    "                feedback.append(f\"{label} detected\")\n",
    "\n",
    "        # Audio feedback\n",
    "        if feedback:\n",
    "            engine.say('. '.join(feedback))\n",
    "            engine.runAndWait()\n",
    "\n",
    "import speech_recognition as sr\n",
    "\n",
    "def voice_command_listener():\n",
    "    r = sr.Recognizer()\n",
    "    with sr.Microphone() as source:\n",
    "        while True:\n",
    "            print(\"Listening...\")\n",
    "            audio = r.listen(source)\n",
    "            try:\n",
    "                text = r.recognize_google(audio).lower()\n",
    "                print(f\"You said: {text}\")  # Print whatever is spoken\n",
    "                \n",
    "                if \"yah kya hai\" in text:\n",
    "                    threading.Thread(target=process_frame, daemon=True).start()\n",
    "                elif \"calibrate\" in text:\n",
    "                    calibrate_depth()\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "\n",
    "# Start listening\n",
    "# voice_command_listener()\n",
    "\n",
    "\n",
    "def calibrate_depth():\n",
    "    global scaling_factor\n",
    "    if not frame_queue.empty():\n",
    "        frame = frame_queue.get()\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_pil = Image.fromarray(frame_rgb)\n",
    "\n",
    "        results = yolo_model(frame)[0]\n",
    "        for box in results.boxes:\n",
    "            class_id = int(box.cls)\n",
    "            label = yolo_model.names[class_id]\n",
    "            if label in RELEVANT_CLASSES:\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                depth_result = depth_estimator(frame_pil)\n",
    "                depth_map = depth_result['predictions']\n",
    "\n",
    "                depth_map = np.array(depth_map)\n",
    "                depth_roi = depth_map[y1:y2, x1:x2]\n",
    "                avg_depth = depth_roi.mean() if depth_roi.size > 0 else 0\n",
    "\n",
    "                if avg_depth > 0:\n",
    "                    scaling_factor = 1 / (avg_depth * CALIBRATION_DISTANCE)\n",
    "                    engine.say(\"Calibration complete\")\n",
    "                    engine.runAndWait()\n",
    "                    return\n",
    "\n",
    "# Start threads\n",
    "camera_thread = threading.Thread(target=camera_capture, daemon=True)\n",
    "voice_thread = threading.Thread(target=voice_command_listener, daemon=True)\n",
    "\n",
    "camera_thread.start()\n",
    "voice_thread.start()\n",
    "\n",
    "# Keep main thread alive efficiently\n",
    "while True:\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
